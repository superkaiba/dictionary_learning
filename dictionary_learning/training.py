"""
Training dictionaries
"""

import json
import os
from collections import defaultdict
import torch as th
from tqdm import tqdm
from warnings import warn
import wandb
import torch.distributed as dist
import torch.nn.parallel as parallel

from .trainers.crosscoder import CrossCoderTrainer, BatchTopKCrossCoderTrainer


def get_stats(
    trainer,
    act: th.Tensor,
    deads_sum: bool = True,
    step: int = None,
    use_threshold: bool = True,
):
    with th.no_grad():
        act, act_hat, f, losslog = trainer.loss(
            act, step=step, logging=True, return_deads=True, use_threshold=use_threshold
        )

    # L0
    l0 = (f != 0).float().detach().cpu().sum(dim=-1).mean().item()

    out = {
        "l0": l0,
        **{f"{k}": v for k, v in losslog.items() if k != "deads"},
    }
    if "deads" in losslog and losslog["deads"] is not None:
        total_feats = losslog["deads"].shape[0]
        out["frac_deads"] = (
            losslog["deads"].sum().item() / total_feats
            if deads_sum
            else losslog["deads"]
        )

    # fraction of variance explained
    if act.dim() == 2:
        # act.shape: [batch, d_model]
        # fraction of variance explained
        total_variance = th.var(act, dim=0).sum()
        residual_variance = th.var(act - act_hat, dim=0).sum()
        frac_variance_explained = 1 - residual_variance / total_variance
    else:
        # act.shape: [batch, layer, d_model]
        total_variance_per_layer = []
        residual_variance_per_layer = []

        for l in range(act_hat.shape[1]):
            total_variance_per_layer.append(th.var(act[:, l, :], dim=0).cpu().sum())
            residual_variance_per_layer.append(
                th.var(act[:, l, :] - act_hat[:, l, :], dim=0).cpu().sum()
            )
            out[f"cl{l}_frac_variance_explained"] = (
                1 - residual_variance_per_layer[l] / total_variance_per_layer[l]
            )
        total_variance = sum(total_variance_per_layer)
        residual_variance = sum(residual_variance_per_layer)
        frac_variance_explained = 1 - residual_variance / total_variance

    out["frac_variance_explained"] = frac_variance_explained.item()
    return out


def log_stats(
    trainer,
    step: int,
    act: th.Tensor,
    activations_split_by_head: bool,
    transcoder: bool,
    stage: str = "train",
    use_threshold: bool = True,
):
    with th.no_grad():
        log = {}
        if activations_split_by_head:  # x.shape: [batch, pos, n_heads, d_head]
            act = act[..., 0, :]
        if not transcoder:
            stats = get_stats(trainer, act, step=step, use_threshold=use_threshold)
            log.update({f"{stage}/{k}": v for k, v in stats.items()})
        else:  # transcoder
            x, x_hat, f, losslog = trainer.loss(act, step=step, logging=True)
            # L0
            l0 = (f != 0).float().sum(dim=-1).mean().item()
            log[f"{stage}/l0"] = l0

        # log parameters from training
        log["step"] = step
        trainer_log = trainer.get_logging_parameters()
        for name, value in trainer_log.items():
            log[f"{stage}/{name}"] = value

        wandb.log(log, step=step)


@th.no_grad()
def run_validation(
    trainer,
    validation_data,
    step: int = None,
    dtype: th.dtype = th.float32,
):
    l0 = []
    frac_variance_explained = []
    frac_variance_explained_per_feature = []
    deads = []
    if isinstance(trainer, CrossCoderTrainer) or isinstance(
        trainer, BatchTopKCrossCoderTrainer
    ):
        frac_variance_explained_per_layer = defaultdict(list)
    for val_step, act in enumerate(tqdm(validation_data, total=len(validation_data))):
        act = act.to(trainer.device).to(dtype)
        stats = get_stats(trainer, act, deads_sum=False, step=step)
        l0.append(stats["l0"])
        if "frac_deads" in stats:
            deads.append(stats["frac_deads"])
        if "frac_variance_explained" in stats:
            frac_variance_explained.append(stats["frac_variance_explained"])
        if "frac_variance_explained_per_feature" in stats:
            frac_variance_explained_per_feature.append(
                stats["frac_variance_explained_per_feature"]
            )

        if isinstance(trainer, (CrossCoderTrainer, BatchTopKCrossCoderTrainer)):
            for l in range(act.shape[1]):
                if f"cl{l}_frac_variance_explained" in stats:
                    frac_variance_explained_per_layer[l].append(
                        stats[f"cl{l}_frac_variance_explained"]
                    )
    log = {}
    if isinstance(trainer, (CrossCoderTrainer, BatchTopKCrossCoderTrainer)):
        dec_norms = trainer.ae.decoder.weight.norm(dim=-1)
        dec_norms_sum = dec_norms.sum(dim=0)
        for layer_idx in range(trainer.ae.decoder.num_layers):
            dec_norm_diff = 0.5 * (
                (2 * dec_norms[layer_idx] - dec_norms_sum)
                / th.maximum(dec_norms[layer_idx], dec_norms_sum - dec_norms[layer_idx])
                + 1
            )
            num_layer_specific_latents = (dec_norm_diff > 0.9).sum().item()
            log[f"val/num_specific_latents_l{layer_idx}"] = num_layer_specific_latents
    if len(deads) > 0:
        log["val/frac_deads"] = th.stack(deads).all(dim=0).float().mean().item()
    if len(l0) > 0:
        log["val/l0"] = th.tensor(l0).mean().item()
    if len(frac_variance_explained) > 0:
        log["val/frac_variance_explained"] = th.tensor(frac_variance_explained).mean()
    if len(frac_variance_explained_per_feature) > 0:
        frac_variance_explained_per_feature = th.stack(
            frac_variance_explained_per_feature
        ).cpu()  # [num_features]
        log["val/frac_variance_explained_per_feature"] = (
            frac_variance_explained_per_feature
        )
    if isinstance(trainer, CrossCoderTrainer) or isinstance(
        trainer, BatchTopKCrossCoderTrainer
    ):
        for l in frac_variance_explained_per_layer:
            log[f"val/cl{l}_frac_variance_explained"] = th.tensor(
                frac_variance_explained_per_layer[l]
            ).mean()
    if step is not None:
        log["step"] = step
    wandb.log(log, step=step)

    return log


def save_model(trainer, checkpoint_name, save_dir):
    os.makedirs(save_dir, exist_ok=True)
    # Handle the case where the model might be compiled
    if hasattr(trainer, "ae"):
        model = trainer.ae
        if hasattr(model, "_orig_mod"):  # Check if model is compiled
            model = model._orig_mod
        # If using DDP, save the module without wrapper
        if isinstance(model, parallel.DistributedDataParallel):
            model = model.module
        th.save(model.state_dict(), os.path.join(save_dir, checkpoint_name))
    else:
        model = trainer.model
        if hasattr(model, "_orig_mod"):  # Check if model is compiled
            model = model._orig_mod
        # If using DDP, save the module without wrapper
        if isinstance(model, parallel.DistributedDataParallel):
            model = model.module
        th.save(model.state_dict(), os.path.join(save_dir, checkpoint_name))


def trainSAE(
    data,
    trainer_config,
    use_wandb=False,
    wandb_entity="",
    wandb_project="",
    steps=None,
    save_steps=None,
    save_dir=None,
    log_steps=None,
    activations_split_by_head=False,
    validate_every_n_steps=None,
    validation_data=None,
    transcoder=False,
    run_cfg={},
    end_of_step_logging_fn=None,
    save_last_eval=True,
    start_of_training_eval=False,
    dtype=th.float32,
    wandb_dir=None,
):
    """
    Train SAE using the given trainer
    """
    assert not (
        validation_data is None and validate_every_n_steps is not None
    ), "Must provide validation data if validate_every_n_steps is not None"

    # Extract distributed training parameters
    distributed = trainer_config.get("distributed", False)
    rank = trainer_config.get("rank", 0)
    world_size = trainer_config.get("world_size", 1)
    
    # Only use wandb on main process if distributed
    if distributed and rank != 0:
        use_wandb = False
    
    trainer_class = trainer_config["trainer"]
    del trainer_config["trainer"]
    
    # Remove distributed parameters from trainer config
    if "distributed" in trainer_config:
        del trainer_config["distributed"]
    if "rank" in trainer_config:
        del trainer_config["rank"]
    if "world_size" in trainer_config:
        del trainer_config["world_size"]
        
    trainer = trainer_class(**trainer_config)

    wandb_config = trainer.config | run_cfg
    if use_wandb:
        wandb.init(
            entity=wandb_entity,
            project=wandb_project,
            config=wandb_config,
            name=wandb_config["wandb_name"],
            mode="disabled" if not use_wandb else "online",
            dir=wandb_dir if wandb_dir is not None else None,
        )

    trainer.model.to(dtype)
    
    # Set up distributed training if enabled
    if distributed:
        # Wrap model with DistributedDataParallel
        local_rank = rank % th.cuda.device_count()
        trainer.model = parallel.DistributedDataParallel(
            trainer.model, 
            device_ids=[local_rank],
            output_device=local_rank
        )

    # make save dir, export config
    if save_dir is not None and (not distributed or rank == 0):
        os.makedirs(save_dir, exist_ok=True)
        # save config
        config = {"trainer": trainer.config}
        try:
            config["buffer"] = data.config
        except Exception as e:
            warn(f"Error saving config: {e}")
            pass
        with open(os.path.join(save_dir, "config.json"), "w") as f:
            json.dump(config, f, indent=4)

    for step, (act, tokens) in enumerate(tqdm(data, total=steps)):
        if step >= steps:
            break

        act = act.to(trainer.device).to(dtype)

        trainer.update(step, act)

        if log_steps is not None and step % log_steps == 0 and use_wandb:
            log_stats(
                trainer,
                step,
                act,
                activations_split_by_head=activations_split_by_head,
                transcoder=transcoder,
            )

        if save_steps is not None and step % save_steps == 0 and save_dir is not None and (not distributed or rank == 0):
            save_model(trainer, f"step_{step}.pt", save_dir)

        if (
            validate_every_n_steps is not None
            and step % validate_every_n_steps == 0
            and use_wandb
        ):
            run_validation(trainer, validation_data, step=step, dtype=dtype)

        if end_of_step_logging_fn is not None:
            end_of_step_logging_fn(step, trainer, act)

    if save_dir is not None and (not distributed or rank == 0):
        save_model(trainer, "final.pt", save_dir)

    if save_last_eval and validation_data is not None and use_wandb:
        run_validation(trainer, validation_data, step=step, dtype=dtype)

    return trainer
